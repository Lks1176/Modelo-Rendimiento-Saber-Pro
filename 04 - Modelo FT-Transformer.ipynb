{"cells":[{"cell_type":"markdown","id":"30d770aa","metadata":{"id":"30d770aa"},"source":["# Entrenamiento multiclase para `RENDIMIENTO_GLOBAL`\n","Este cuaderno está optimizado para Google Colab T4 (GPU) e implementa un pipeline completo para comparar boosting (LightGBM), FT-Transformer y un MLP residual rápido sobre datos tabulares ya preprocesados. Se incluyen Optuna, validación estratificada (K=5), métricas detalladas y artefactos exportables.\n","\n","> **Atajos operativos**: Ejecuta las celdas en orden, ajusta `N_TRIALS_*` según el tiempo disponible y revisa el resumen final para elegir el modelo a desplegar."]},{"cell_type":"code","execution_count":null,"id":"71d4a95d","metadata":{"id":"71d4a95d"},"outputs":[],"source":["%%capture\n","# Dependencias necesarias (ejecutar una vez por sesión de Colab T4)\n","%pip install -q pytorch-lightning==2.4.0 optuna==3.6.1 lightgbm==4.3.0 catboost==1.2.5 \\\n","    tabpfn==0.1.10 rtdl==0.0.13 torchmetrics==1.4.0 seaborn==0.13.2 matplotlib==3.8.4"]},{"cell_type":"code","execution_count":null,"id":"f9aed8da","metadata":{"id":"f9aed8da"},"outputs":[],"source":["import gc\n","import json\n","import math\n","import random\n","from collections import defaultdict\n","from pathlib import Path\n","\n","import joblib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import optuna\n","import pandas as pd\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.model_selection import StratifiedKFold\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","import lightgbm as lgb\n","from tabpfn import TabPFNClassifier\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n","from pytorch_lightning.loggers import CSVLogger\n","from torchmetrics.classification import MulticlassAccuracy\n","\n","pl.seed_everything(42, workers=True)\n","optuna.logging.set_verbosity(optuna.logging.WARNING)"]},{"cell_type":"code","execution_count":null,"id":"abec39d4","metadata":{"id":"abec39d4"},"outputs":[],"source":["# Configuración global y rutas relevantes\n","DATA_PATH = Path(\"/mnt/data/processed_train.parquet\")\n","PIPELINE_PATH = Path(\"/mnt/data/preprocessing_pipeline.joblib\")\n","TARGET_COL = \"RENDIMIENTO_GLOBAL\"\n","CLASS_NAMES = [\"alto\", \"medio-alto\", \"medio-bajo\", \"bajo\"]\n","CLASS2IDX = {cls: idx for idx, cls in enumerate(CLASS_NAMES)}\n","IDX2CLASS = {idx: cls for cls, idx in CLASS2IDX.items()}\n","\n","N_SPLITS = 5\n","RANDOM_STATE = 42\n","N_JOBS = 2\n","BATCH_SIZE_SPACE = [512, 1024, 2048, 4096]\n","ARTIFACT_DIR = Path(\"./artifacts\")\n","ARTIFACT_DIR.mkdir(exist_ok=True, parents=True)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Device: {device} | Artifacts: {ARTIFACT_DIR.resolve()}\")"]},{"cell_type":"code","execution_count":null,"id":"9c8917ab","metadata":{"id":"9c8917ab"},"outputs":[],"source":["# Utilidades compartidas\n","\n","def set_seed(seed: int = RANDOM_STATE) -> None:\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","\n","def make_dataloaders(X: np.ndarray, y: np.ndarray, train_idx: np.ndarray,\n","                     val_idx: np.ndarray, batch_size: int, num_workers: int = 2):\n","    \"\"\"Construye DataLoaders tensoriales con pin_memory para GPU.\"\"\"\n","    X_train = torch.from_numpy(X[train_idx]).float()\n","    y_train = torch.from_numpy(y[train_idx]).long()\n","    X_val = torch.from_numpy(X[val_idx]).float()\n","    y_val = torch.from_numpy(y[val_idx]).long()\n","\n","    train_loader = DataLoader(\n","        TensorDataset(X_train, y_train),\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        pin_memory=(device == \"cuda\"),\n","        drop_last=False,\n","    )\n","    val_loader = DataLoader(\n","        TensorDataset(X_val, y_val),\n","        batch_size=min(batch_size, 2048),\n","        shuffle=False,\n","        num_workers=num_workers,\n","        pin_memory=(device == \"cuda\"),\n","    )\n","    return train_loader, val_loader\n","\n","\n","def collect_fold_metrics(name: str, fold_scores: list[dict]) -> pd.DataFrame:\n","    df = pd.DataFrame(fold_scores)\n","    summary = {\n","        \"model\": name,\n","        \"mean_acc\": df[\"accuracy\"].mean(),\n","        \"std_acc\": df[\"accuracy\"].std(ddof=0),\n","        \"min_acc\": df[\"accuracy\"].min(),\n","        \"max_acc\": df[\"accuracy\"].max(),\n","    }\n","    return df, summary\n","\n","\n","def logits_to_numpy(logits_list):\n","    return torch.cat(logits_list).softmax(dim=1).cpu().numpy()\n","\n","\n","def describe_class_balance(labels: np.ndarray):\n","    counts = pd.Series(labels).value_counts().sort_index()\n","    display(counts.rename(index=IDX2CLASS))\n"]},{"cell_type":"code","execution_count":null,"id":"43c2f7a4","metadata":{"id":"43c2f7a4"},"outputs":[],"source":["# Carga de datos preprocesados y pipeline de referencia\n","assert DATA_PATH.exists(), f\"No se encontró {DATA_PATH}\"\n","assert PIPELINE_PATH.exists(), f\"No se encontró {PIPELINE_PATH}\"\n","\n","pipeline = joblib.load(PIPELINE_PATH)\n","df = pd.read_parquet(DATA_PATH)\n","print(f\"Shape: {df.shape} | Memoria ~{df.memory_usage().sum() / 1e6:.1f} MB\")\n","\n","y = df[TARGET_COL].map(CLASS2IDX).to_numpy(dtype=np.int64)\n","X = df.drop(columns=[TARGET_COL]).to_numpy(dtype=np.float32)\n","\n","describe_class_balance(y)\n","print(f\"Feature dims: {X.shape[1]}\")\n","\n","del df\n","_ = gc.collect()"]},{"cell_type":"code","execution_count":null,"id":"f7c3c694","metadata":{"id":"f7c3c694"},"outputs":[],"source":["# Definición de folds estratificados K=5\n","skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n","folds = list(skf.split(X, y))\n","print(f\"Folds preparados: {len(folds)}\")"]},{"cell_type":"code","execution_count":null,"id":"16c45239","metadata":{"id":"16c45239"},"outputs":[],"source":["# --- LightGBM + Optuna -----------------------------------------------------\n","\n","def tune_lightgbm(X, y, folds, n_trials: int = 20):\n","    def objective(trial: optuna.Trial) -> float:\n","        params = {\n","            \"objective\": \"multiclass\",\n","            \"num_class\": len(CLASS_NAMES),\n","            \"metric\": \"multi_logloss\",\n","            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n","            \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255),\n","            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n","            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n","            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n","            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 200),\n","            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n","            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n","            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n","            \"num_threads\": N_JOBS,\n","        }\n","        fold_accs = []\n","        for fold_id, (tr_idx, val_idx) in enumerate(folds[:2]):  # usar 2 folds para acelerar la búsqueda\n","            lgb_train = lgb.Dataset(X[tr_idx], label=y[tr_idx])\n","            lgb_val = lgb.Dataset(X[val_idx], label=y[val_idx])\n","            booster = lgb.train(\n","                params,\n","                lgb_train,\n","                valid_sets=[lgb_val],\n","                num_boost_round=3000,\n","                callbacks=[\n","                    lgb.early_stopping(stopping_rounds=200, verbose=False),\n","                ],\n","            )\n","            preds = booster.predict(X[val_idx], num_iteration=booster.best_iteration)\n","            fold_accs.append(accuracy_score(y[val_idx], preds.argmax(axis=1)))\n","        return float(np.mean(fold_accs))\n","\n","    study = optuna.create_study(direction=\"maximize\", study_name=\"lgbm_study\")\n","    study.optimize(objective, n_trials=n_trials, n_jobs=N_JOBS, show_progress_bar=True)\n","    best_params = study.best_params\n","    best_params.update({\n","        \"objective\": \"multiclass\",\n","        \"num_class\": len(CLASS_NAMES),\n","        \"metric\": \"multi_logloss\",\n","        \"num_threads\": N_JOBS,\n","    })\n","    return study, best_params\n","\n","\n","def train_lightgbm_cv(params: dict, X: np.ndarray, y: np.ndarray, folds):\n","    oof_pred = np.zeros((len(y), len(CLASS_NAMES)), dtype=np.float32)\n","    fold_scores = []\n","    model_paths = []\n","    for fold_id, (tr_idx, val_idx) in enumerate(folds):\n","        print(f\"[LightGBM] Fold {fold_id}\")\n","        lgb_train = lgb.Dataset(X[tr_idx], label=y[tr_idx])\n","        lgb_val = lgb.Dataset(X[val_idx], label=y[val_idx])\n","        booster = lgb.train(\n","            params,\n","            lgb_train,\n","            valid_sets=[lgb_train, lgb_val],\n","            valid_names=[\"train\", \"valid\"],\n","            num_boost_round=5000,\n","            callbacks=[\n","                lgb.early_stopping(stopping_rounds=300, verbose=False),\n","            ],\n","        )\n","        preds = booster.predict(X[val_idx], num_iteration=booster.best_iteration)\n","        oof_pred[val_idx] = preds\n","        acc = accuracy_score(y[val_idx], preds.argmax(axis=1))\n","        fold_scores.append({\n","            \"fold\": fold_id,\n","            \"accuracy\": acc,\n","            \"best_iteration\": booster.best_iteration,\n","        })\n","        model_path = ARTIFACT_DIR / f\"lightgbm_fold{fold_id}.txt\"\n","        booster.save_model(str(model_path))\n","        model_paths.append(model_path)\n","    return {\n","        \"name\": \"LightGBM\",\n","        \"best_params\": params,\n","        \"fold_metrics\": fold_scores,\n","        \"oof_predictions\": oof_pred,\n","        \"model_paths\": model_paths,\n","    }\n"]},{"cell_type":"code","execution_count":null,"id":"f3ba7923","metadata":{"id":"f3ba7923"},"outputs":[],"source":["# Tuning y entrenamiento LightGBM\n","N_TRIALS_LGB = 20\n","lgb_study, lgb_best_params = tune_lightgbm(X, y, folds, n_trials=N_TRIALS_LGB)\n","print(f\"LightGBM best params: {json.dumps(lgb_best_params, indent=2)}\")\n","\n","lgb_results = train_lightgbm_cv(lgb_best_params, X, y, folds)\n","lgb_fold_df, lgb_summary = collect_fold_metrics(lgb_results[\"name\"], lgb_results[\"fold_metrics\"])\n","lgb_fold_df"]},{"cell_type":"code","execution_count":null,"id":"81e79c31","metadata":{"id":"81e79c31"},"outputs":[],"source":["# --- FT-Transformer Lightning Module ----------------------------------------\n","\n","class NumericalFeatureTokenizer(nn.Module):\n","    def __init__(self, input_dim: int, d_token: int):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.empty(input_dim, d_token))\n","        self.bias = nn.Parameter(torch.zeros(input_dim, d_token))\n","        nn.init.xavier_uniform_(self.weight)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # x: (batch, dim) -> tokens: (batch, dim, d_token)\n","        return x.unsqueeze(-1) * self.weight.unsqueeze(0) + self.bias.unsqueeze(0)\n","\n","\n","class FTTransformerBackbone(nn.Module):\n","    def __init__(self, input_dim: int, n_classes: int, d_token: int = 192,\n","                 n_layers: int = 3, n_heads: int = 8, dropout: float = 0.2,\n","                 ffn_dropout: float = 0.2):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.tokenizer = NumericalFeatureTokenizer(input_dim, d_token)\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_token,\n","            nhead=n_heads,\n","            batch_first=True,\n","            dropout=dropout,\n","            dim_feedforward=d_token * 4,\n","            activation=\"gelu\",\n","        )\n","        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n","        self.pos_embedding = nn.Parameter(torch.randn(1, input_dim + 1, d_token) * 0.02)\n","        self.norm = nn.LayerNorm(d_token)\n","        self.head = nn.Linear(d_token, n_classes)\n","        self.ffn_dropout = nn.Dropout(ffn_dropout)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        tokens = self.tokenizer(x)\n","        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n","        tokens = torch.cat([cls_tokens, tokens], dim=1) + self.pos_embedding[:, : tokens.size(1), :]\n","        encoded = self.encoder(tokens)\n","        cls_rep = self.norm(encoded[:, 0])\n","        cls_rep = self.ffn_dropout(cls_rep)\n","        return self.head(cls_rep)\n","\n","\n","class FTTransformerModule(pl.LightningModule):\n","    def __init__(self, input_dim: int, n_classes: int, d_token: int, n_layers: int,\n","                 n_heads: int, dropout: float, ffn_dropout: float,\n","                 lr: float, weight_decay: float):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.model = FTTransformerBackbone(\n","            input_dim=input_dim,\n","            n_classes=n_classes,\n","            d_token=d_token,\n","            n_layers=n_layers,\n","            n_heads=n_heads,\n","            dropout=dropout,\n","            ffn_dropout=ffn_dropout,\n","        )\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.val_acc = MulticlassAccuracy(num_classes=n_classes).to(device)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = self.criterion(logits, y)\n","        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = self.criterion(logits, y)\n","        preds = torch.argmax(logits, dim=1)\n","        acc = (preds == y).float().mean()\n","        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n","        self.log(\"val_acc\", acc, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n","        return {\"logits\": logits.detach(), \"targets\": y.detach()}\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n","        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n","        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n"]},{"cell_type":"code","execution_count":null,"id":"25b32070","metadata":{"id":"25b32070"},"outputs":[],"source":["# Búsqueda Optuna para FT-Transformer\n","\n","def tune_ft_transformer(X, y, folds, n_trials: int = 15):\n","    input_dim = X.shape[1]\n","\n","    def objective(trial: optuna.Trial) -> float:\n","        params = {\n","            \"d_token\": trial.suggest_categorical(\"d_token\", [128, 192, 256]),\n","            \"n_layers\": trial.suggest_int(\"n_layers\", 2, 4),\n","            \"n_heads\": trial.suggest_categorical(\"n_heads\", [4, 8]),\n","            \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.3),\n","            \"ffn_dropout\": trial.suggest_float(\"ffn_dropout\", 0.0, 0.3),\n","            \"lr\": trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True),\n","            \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True),\n","            \"batch_size\": trial.suggest_categorical(\"batch_size\", BATCH_SIZE_SPACE),\n","            \"max_epochs\": trial.suggest_int(\"max_epochs\", 15, 35),\n","        }\n","        train_idx, val_idx = folds[0]\n","        train_loader, val_loader = make_dataloaders(X, y, train_idx, val_idx, params[\"batch_size\"])\n","        module = FTTransformerModule(\n","            input_dim=input_dim,\n","            n_classes=len(CLASS_NAMES),\n","            d_token=params[\"d_token\"],\n","            n_layers=params[\"n_layers\"],\n","            n_heads=params[\"n_heads\"],\n","            dropout=params[\"dropout\"],\n","            ffn_dropout=params[\"ffn_dropout\"],\n","            lr=params[\"lr\"],\n","            weight_decay=params[\"weight_decay\"],\n","        )\n","        callbacks = [\n","            EarlyStopping(monitor=\"val_acc\", patience=5, mode=\"max\"),\n","        ]\n","        trainer = pl.Trainer(\n","            accelerator=\"gpu\" if device == \"cuda\" else \"cpu\",\n","            devices=1,\n","            precision=\"16-mixed\" if device == \"cuda\" else \"32-true\",\n","            max_epochs=params[\"max_epochs\"],\n","            enable_checkpointing=False,\n","            logger=False,\n","            enable_progress_bar=False,\n","        )\n","        trainer.fit(module, train_loader, val_loader)\n","        val_acc = float(trainer.callback_metrics.get(\"val_acc\", torch.tensor(0.0)).cpu())\n","        torch.cuda.empty_cache()\n","        return val_acc\n","\n","    study = optuna.create_study(direction=\"maximize\", study_name=\"ft_transformer\")\n","    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n","    return study, study.best_params\n","\n","\n","def train_ft_transformer_cv(best_params: dict, X, y, folds):\n","    params = best_params.copy()\n","    batch_size = params.pop(\"batch_size\")\n","    max_epochs = params.pop(\"max_epochs\")\n","    input_dim = X.shape[1]\n","\n","    oof_pred = np.zeros((len(y), len(CLASS_NAMES)), dtype=np.float32)\n","    fold_scores = []\n","    checkpoints = []\n","\n","    for fold_id, (tr_idx, val_idx) in enumerate(folds):\n","        print(f\"[FT-Transformer] Fold {fold_id}\")\n","        train_loader, val_loader = make_dataloaders(X, y, tr_idx, val_idx, batch_size)\n","        module = FTTransformerModule(input_dim=input_dim, n_classes=len(CLASS_NAMES), **params)\n","        callbacks = [\n","            EarlyStopping(monitor=\"val_acc\", patience=6, mode=\"max\"),\n","            ModelCheckpoint(\n","                dirpath=ARTIFACT_DIR / f\"ft_transformer_fold{fold_id}\",\n","                filename=\"ft-transformer-{epoch:02d}-{val_acc:.4f}\",\n","                monitor=\"val_acc\",\n","                mode=\"max\",\n","                save_top_k=1,\n","            ),\n","            LearningRateMonitor(logging_interval=\"epoch\"),\n","        ]\n","        logger = CSVLogger(save_dir=ARTIFACT_DIR / f\"logs_ft_fold{fold_id}\", name=\"ft\")\n","        trainer = pl.Trainer(\n","            accelerator=\"gpu\" if device == \"cuda\" else \"cpu\",\n","            devices=1,\n","            precision=\"16-mixed\" if device == \"cuda\" else \"32-true\",\n","            max_epochs=max_epochs,\n","            callbacks=callbacks,\n","            logger=logger,\n","            gradient_clip_val=1.0,\n","            deterministic=False,\n","        )\n","        trainer.fit(module, train_loader, val_loader)\n","        best_ckpt = callbacks[1].best_model_path\n","        checkpoints.append(best_ckpt)\n","        best_model = FTTransformerModule.load_from_checkpoint(best_ckpt)\n","        best_model.to(device)\n","        best_model.eval()\n","        val_logits = []\n","        with torch.no_grad():\n","            for xb, yb in val_loader:\n","                xb = xb.to(device)\n","                logits = best_model(xb)\n","                val_logits.append(logits.cpu())\n","        probs = torch.cat(val_logits).softmax(dim=1).numpy()\n","        oof_pred[val_idx] = probs\n","        preds = probs.argmax(axis=1)\n","        acc = accuracy_score(y[val_idx], preds)\n","        fold_scores.append({\"fold\": fold_id, \"accuracy\": acc})\n","        del best_model\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","    return {\n","        \"name\": \"FT-Transformer\",\n","        \"best_params\": best_params,\n","        \"fold_metrics\": fold_scores,\n","        \"oof_predictions\": oof_pred,\n","        \"model_paths\": checkpoints,\n","    }\n"]},{"cell_type":"code","execution_count":null,"id":"392c9764","metadata":{"id":"392c9764"},"outputs":[],"source":["# Tuning y entrenamiento FT-Transformer\n","N_TRIALS_FT = 15\n","ft_study, ft_best_params = tune_ft_transformer(X, y, folds, n_trials=N_TRIALS_FT)\n","print(f\"FT params: {json.dumps(ft_best_params, indent=2)}\")\n","\n","ft_results = train_ft_transformer_cv(ft_best_params, X, y, folds)\n","ft_fold_df, ft_summary = collect_fold_metrics(ft_results[\"name\"], ft_results[\"fold_metrics\"])\n","ft_fold_df"]},{"cell_type":"code","execution_count":null,"id":"aef612c1","metadata":{"id":"aef612c1"},"outputs":[],"source":["# --- MLP Residual (ResNet-like) ---------------------------------------------\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, dim: int, dropout: float, residual_dropout: float):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(dim, dim),\n","            nn.Dropout(residual_dropout),\n","        )\n","\n","    def forward(self, x):\n","        return x + self.block(x)\n","\n","\n","class ResidualMLP(nn.Module):\n","    def __init__(self, input_dim: int, hidden_dim: int, depth: int,\n","                 dropout: float, residual_dropout: float, n_classes: int):\n","        super().__init__()\n","        self.input_proj = nn.Sequential(\n","            nn.LayerNorm(input_dim),\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","        )\n","        self.blocks = nn.ModuleList(\n","            [ResidualBlock(hidden_dim, dropout, residual_dropout) for _ in range(depth)]\n","        )\n","        self.head = nn.Sequential(\n","            nn.LayerNorm(hidden_dim),\n","            nn.Linear(hidden_dim, n_classes),\n","        )\n","\n","    def forward(self, x):\n","        h = self.input_proj(x)\n","        for block in self.blocks:\n","            h = block(h)\n","        return self.head(h)\n","\n","\n","class ResidualMLPModule(pl.LightningModule):\n","    def __init__(self, input_dim: int, n_classes: int, hidden_dim: int, depth: int,\n","                 dropout: float, residual_dropout: float, lr: float, weight_decay: float):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.model = ResidualMLP(input_dim, hidden_dim, depth, dropout, residual_dropout, n_classes)\n","        self.criterion = nn.CrossEntropyLoss()\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = self.criterion(logits, y)\n","        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = self.criterion(logits, y)\n","        preds = torch.argmax(logits, dim=1)\n","        acc = (preds == y).float().mean()\n","        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True, sync_dist=True)\n","        self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True, sync_dist=True)\n","        return {\"logits\": logits.detach(), \"targets\": y.detach()}\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n","        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n","        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n"]},{"cell_type":"code","execution_count":null,"id":"31deb855","metadata":{"id":"31deb855"},"outputs":[],"source":["# Optuna + entrenamiento para el MLP residual\n","\n","def tune_residual_mlp(X, y, folds, n_trials: int = 15):\n","    input_dim = X.shape[1]\n","\n","    def objective(trial: optuna.Trial) -> float:\n","        params = {\n","            \"hidden_dim\": trial.suggest_categorical(\"hidden_dim\", [256, 384, 512, 640]),\n","            \"depth\": trial.suggest_int(\"depth\", 3, 6),\n","            \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.3),\n","            \"residual_dropout\": trial.suggest_float(\"residual_dropout\", 0.0, 0.2),\n","            \"lr\": trial.suggest_float(\"lr\", 5e-4, 5e-3, log=True),\n","            \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True),\n","            \"batch_size\": trial.suggest_categorical(\"batch_size\", BATCH_SIZE_SPACE),\n","            \"max_epochs\": trial.suggest_int(\"max_epochs\", 12, 30),\n","        }\n","        train_idx, val_idx = folds[0]\n","        train_loader, val_loader = make_dataloaders(X, y, train_idx, val_idx, params[\"batch_size\"])\n","        module = ResidualMLPModule(\n","            input_dim=input_dim,\n","            n_classes=len(CLASS_NAMES),\n","            hidden_dim=params[\"hidden_dim\"],\n","            depth=params[\"depth\"],\n","            dropout=params[\"dropout\"],\n","            residual_dropout=params[\"residual_dropout\"],\n","            lr=params[\"lr\"],\n","            weight_decay=params[\"weight_decay\"],\n","        )\n","        trainer = pl.Trainer(\n","            accelerator=\"gpu\" if device == \"cuda\" else \"cpu\",\n","            devices=1,\n","            precision=\"16-mixed\" if device == \"cuda\" else \"32-true\",\n","            max_epochs=params[\"max_epochs\"],\n","            enable_checkpointing=False,\n","            logger=False,\n","            enable_progress_bar=False,\n","        )\n","        trainer.fit(module, train_loader, val_loader)\n","        val_acc = float(trainer.callback_metrics.get(\"val_acc\", torch.tensor(0.0)).cpu())\n","        torch.cuda.empty_cache()\n","        return val_acc\n","\n","    study = optuna.create_study(direction=\"maximize\", study_name=\"residual_mlp\")\n","    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n","    return study, study.best_params\n","\n","\n","def train_residual_mlp_cv(best_params: dict, X, y, folds):\n","    params = best_params.copy()\n","    batch_size = params.pop(\"batch_size\")\n","    max_epochs = params.pop(\"max_epochs\")\n","    input_dim = X.shape[1]\n","\n","    oof_pred = np.zeros((len(y), len(CLASS_NAMES)), dtype=np.float32)\n","    fold_scores = []\n","    checkpoints = []\n","\n","    for fold_id, (tr_idx, val_idx) in enumerate(folds):\n","        print(f\"[ResidualMLP] Fold {fold_id}\")\n","        train_loader, val_loader = make_dataloaders(X, y, tr_idx, val_idx, batch_size)\n","        module = ResidualMLPModule(input_dim=input_dim, n_classes=len(CLASS_NAMES), **params)\n","        callbacks = [\n","            EarlyStopping(monitor=\"val_acc\", patience=5, mode=\"max\"),\n","            ModelCheckpoint(\n","                dirpath=ARTIFACT_DIR / f\"residual_mlp_fold{fold_id}\",\n","                filename=\"res-mlp-{epoch:02d}-{val_acc:.4f}\",\n","                monitor=\"val_acc\",\n","                mode=\"max\",\n","                save_top_k=1,\n","            ),\n","        ]\n","        logger = CSVLogger(save_dir=ARTIFACT_DIR / f\"logs_res_fold{fold_id}\", name=\"resmlp\")\n","        trainer = pl.Trainer(\n","            accelerator=\"gpu\" if device == \"cuda\" else \"cpu\",\n","            devices=1,\n","            precision=\"16-mixed\" if device == \"cuda\" else \"32-true\",\n","            max_epochs=max_epochs,\n","            callbacks=callbacks,\n","            logger=logger,\n","            gradient_clip_val=1.0,\n","        )\n","        trainer.fit(module, train_loader, val_loader)\n","        best_ckpt = callbacks[1].best_model_path\n","        checkpoints.append(best_ckpt)\n","        best_model = ResidualMLPModule.load_from_checkpoint(best_ckpt)\n","        best_model.to(device).eval()\n","        val_logits = []\n","        with torch.no_grad():\n","            for xb, yb in val_loader:\n","                xb = xb.to(device)\n","                val_logits.append(best_model(xb).cpu())\n","        probs = torch.cat(val_logits).softmax(dim=1).numpy()\n","        oof_pred[val_idx] = probs\n","        preds = probs.argmax(axis=1)\n","        fold_scores.append({\"fold\": fold_id, \"accuracy\": accuracy_score(y[val_idx], preds)})\n","        del best_model\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","    return {\n","        \"name\": \"ResidualMLP\",\n","        \"best_params\": best_params,\n","        \"fold_metrics\": fold_scores,\n","        \"oof_predictions\": oof_pred,\n","        \"model_paths\": checkpoints,\n","    }\n"]},{"cell_type":"code","execution_count":null,"id":"5f2e34f8","metadata":{"id":"5f2e34f8"},"outputs":[],"source":["# Tuning y entrenamiento del MLP residual\n","N_TRIALS_RES = 15\n","res_study, res_best_params = tune_residual_mlp(X, y, folds, n_trials=N_TRIALS_RES)\n","print(f\"Residual params: {json.dumps(res_best_params, indent=2)}\")\n","\n","res_results = train_residual_mlp_cv(res_best_params, X, y, folds)\n","res_fold_df, res_summary = collect_fold_metrics(res_results[\"name\"], res_results[\"fold_metrics\"])\n","res_fold_df"]},{"cell_type":"code","execution_count":null,"id":"b422b8e5","metadata":{"id":"b422b8e5"},"outputs":[],"source":["# --- TabPFN baseline (subset para rapidez) -----------------------------------\n","\n","def evaluate_tabpfn(X, y, folds, max_train_samples: int = 8000):\n","    accs = []\n","    for fold_id, (tr_idx, val_idx) in enumerate(folds):\n","        print(f\"[TabPFN] Fold {fold_id}\")\n","        sampled_idx = np.random.choice(tr_idx, size=min(max_train_samples, len(tr_idx)), replace=False)\n","        clf = TabPFNClassifier(\n","            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n","            N_ensemble_configurations=32,\n","            batch_size=256,\n","        )\n","        clf.fit(X[sampled_idx], y[sampled_idx])\n","        preds = clf.predict(X[val_idx])\n","        acc = accuracy_score(y[val_idx], preds)\n","        accs.append({\"fold\": fold_id, \"accuracy\": acc})\n","    return {\n","        \"name\": \"TabPFN (subset)\",\n","        \"fold_metrics\": accs,\n","        \"oof_predictions\": None,\n","    }\n","\n","\n","tabpfn_results = evaluate_tabpfn(X, y, folds)\n","tabpfn_fold_df, tabpfn_summary = collect_fold_metrics(tabpfn_results[\"name\"], tabpfn_results[\"fold_metrics\"])\n","tabpfn_fold_df"]},{"cell_type":"code","execution_count":null,"id":"92f51ef1","metadata":{"id":"92f51ef1"},"outputs":[],"source":["# Resumen comparativo de modelos\n","result_summaries = [lgb_summary, ft_summary, res_summary, tabpfn_summary]\n","summary_df = pd.DataFrame(result_summaries).sort_values(by=\"mean_acc\", ascending=False)\n","summary_df"]},{"cell_type":"code","execution_count":null,"id":"aa54e630","metadata":{"id":"aa54e630"},"outputs":[],"source":["# Matriz de confusión y reporte para el mejor modelo disponible\n","model_lookup = {res[\"name\"]: res for res in [lgb_results, ft_results, res_results]}\n","best_name = None\n","for _, row in summary_df.iterrows():\n","    candidate = model_lookup.get(row[\"model\"])\n","    if candidate and candidate.get(\"oof_predictions\") is not None:\n","        best_name = row[\"model\"]\n","        break\n","assert best_name is not None, \"No hay modelo con predicciones OOF\"\n","\n","best_result = model_lookup[best_name]\n","y_pred = best_result[\"oof_predictions\"].argmax(axis=1)\n","acc = accuracy_score(y, y_pred)\n","print(f\"Modelo seleccionado: {best_name} | Acc global OOF: {acc:.4f}\")\n","\n","conf_mat = confusion_matrix(y, y_pred)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n","plt.xlabel(\"Predicción\")\n","plt.ylabel(\"Real\")\n","plt.title(f\"Matriz de confusión - {best_name}\")\n","plt.tight_layout()\n","plt.show()\n","\n","report = classification_report(y, y_pred, target_names=CLASS_NAMES, output_dict=False)\n","print(report)"]},{"cell_type":"code","execution_count":null,"id":"8c575ad1","metadata":{"id":"8c575ad1"},"outputs":[],"source":["# Ejemplo de exportación + inferencia (LightGBM como ejemplo)\n","joblib.dump(CLASS_NAMES, ARTIFACT_DIR / \"class_names.joblib\")\n","print(f\"Clases guardadas en {ARTIFACT_DIR}\")\n","\n","lightgbm_models = [str(p) for p in lgb_results[\"model_paths\"]]\n","print(\"Modelos LightGBM:\", lightgbm_models)\n","\n","pipeline = joblib.load(PIPELINE_PATH)\n","\n","def predict_with_lightgbm(raw_df: pd.DataFrame) -> pd.DataFrame:\n","    processed = pipeline.transform(raw_df)\n","    processed = np.asarray(processed, dtype=np.float32)\n","    probas = np.zeros((processed.shape[0], len(CLASS_NAMES)), dtype=np.float32)\n","    for model_path in lightgbm_models:\n","        booster = lgb.Booster(model_file=model_path)\n","        probas += booster.predict(processed)\n","    probas /= len(lightgbm_models)\n","    preds = probas.argmax(axis=1)\n","    result = raw_df.copy()\n","    result[\"RENDIMIENTO_GLOBAL_PRED\"] = [IDX2CLASS[idx] for idx in preds]\n","    result[[f\"proba_{cls}\" for cls in CLASS_NAMES]] = probas\n","    return result\n","\n","# Ejemplo de uso (suponiendo raw_df con las columnas originales sin descartar):\n","# sample_raw = some_raw_df.head(3)\n","# prediction_df = predict_with_lightgbm(sample_raw)\n","# display(prediction_df)"]},{"cell_type":"markdown","id":"80b47fbf","metadata":{"id":"80b47fbf"},"source":["## Notas prácticas y bibliografía resumida\n","- **Boosting vs. deep tabular**: LightGBM/CatBoost siguen dominando con datos medianos-grandes, especialmente cuando las features ya fueron cuidadosamente codificadas. Requiere poca ingeniería de hiperparámetros y ofrece interpretabilidad vía importancia de variables.\n","- **FT-Transformer / SAINT**: útiles cuando hay fuertes interacciones no lineales entre atributos heterogéneos y se dispone de GPU. Regulariza con dropout y weight decay; usa lotes grandes (512–4096) y AMP para saturar la T4.\n","- **ResNet tabular**: baseline estable para escenarios donde se quiera una red poco compleja y rápida de ajustar; añadir stochastic depth o mixup ayuda si aparece overfitting.\n","- **TabPFN**: excelente para prototipar cuando el dataset es pequeño (<50k muestras) o se requiere una predicción rápida sin tuning, pero escala de manera cuadrática por lo que aquí se limita a un muestreo.\n","- **Regularización**: prioriza early stopping + bagging en boosting; en deep models combina dropout, weight decay y augmentation (CutMix tabular) si la precisión se estanca.\n","- **Reporte final**: comunica accuracy macro, matriz de confusión, curvas de calibración/Roc por clase (si aplican) y tiempos de entrenamiento por modelo.\n","\n","**Bibliografía 2020–2025**\n","1. Gorishniy et al., *Revisiting Deep Learning Models for Tabular Data* (NeurIPS 2021). https://arxiv.org/abs/2106.11959\n","2. Gorishniy et al., *FT-Transformer: Fast and Accurate Modeling of Tabular Data* (ICML 2021 Workshop). https://arxiv.org/abs/2106.01126\n","3. Somepalli et al., *SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pretraining* (NeurIPS 2021). https://arxiv.org/abs/2106.01342\n","4. Hollmann et al., *TabPFN: A Transformer that Solves Small Tabular Classification Problems in a Second* (NeurIPS 2022). https://arxiv.org/abs/2207.01848\n","5. Misra et al., *A Survey on Deep Learning for Tabular Data* (ACM Computing Surveys 2023). https://arxiv.org/abs/2207.07454"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[{"file_id":"10pfwrHuvl7mjlQt1C5Vu4efRPGgEkjDU","timestamp":1764165750668}]}},"nbformat":4,"nbformat_minor":5}